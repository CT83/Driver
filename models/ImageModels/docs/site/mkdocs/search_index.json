{
    "docs": [
        {
            "location": "/", 
            "text": "Motivation\n#\n\n\nLearn to build and experiment with well-known Image Processing Neural Network Models. As yet, there is no intention to train or run the models. Only served as a learning exercise to understand how these models are built and how to use the Keras Functional API.\n\n\nThe second reason is that I do not have enough computing resource to fully train such a models with the computing resource I have currently available to not require the large amount of training time.\n\n\n\n\nWarning\n\n\nModels configuration may differ slightly from the original implementation\n\n\n\n\nIntention\n#\n\n\nThe purpose of the repository is to re-create ImageNet winners in Keras and to utilize their pretrainied model weights. The folllowing models will be recreated:\n\n\n\n\nAlexNet\n\n\nCaffe version\n\n\nAdded 9/5/2016\n\n\nAlexnet.py\n\n\n\n\n\n\nAlexNet\n\n\nFrom Original Paper Diagram\n\n\nAdded 9/5/2016\n\n\nAlexNet_Unmodified.py\n\n\n\n\n\n\nCaffeNet\n\n\nCaffe version\n\n\nAdded 11/5/2016\n\n\nCaffeNet.py\n\n\n\n\n\n\nVGG-19\n\n\nCaffe Version\n\n\nAdded 9/5/2016\n\n\nVGG-19.py\n\n\n\n\n\n\nGoogLeNet\n\n\nAs described in Original paper\n\n\nAdded 11/5/2016\n\n\n\n\n\n\n\n\nThe next intention is to check if I can setup Tensorflow distributed training and Tensorflow Serving, untrained models will be used for this.\n\n\nDependencies\n#\n\n\n\n\nKeras\n\n\nTheano / Tensorflow\n\n\nMatplotlib\n\n\nNumpy\n\n\nPydot\n\n\n\n\nDirectory Structure\n#\n\n\n/ImageNetModels\n    /KerasLayers\n        Custom_Layers.py - Contains LRN2D layer\n    /Model\n        *.png - pydot visulisations of each of the models\n        *.txt - outputs from building the models\n    /Serving - reserved for the exported Tensorflow models\n    /docs - MKDocs files\n    *.py - Keras Models", 
            "title": "Overview"
        }, 
        {
            "location": "/#motivation", 
            "text": "Learn to build and experiment with well-known Image Processing Neural Network Models. As yet, there is no intention to train or run the models. Only served as a learning exercise to understand how these models are built and how to use the Keras Functional API.  The second reason is that I do not have enough computing resource to fully train such a models with the computing resource I have currently available to not require the large amount of training time.   Warning  Models configuration may differ slightly from the original implementation", 
            "title": "Motivation"
        }, 
        {
            "location": "/#intention", 
            "text": "The purpose of the repository is to re-create ImageNet winners in Keras and to utilize their pretrainied model weights. The folllowing models will be recreated:   AlexNet  Caffe version  Added 9/5/2016  Alexnet.py    AlexNet  From Original Paper Diagram  Added 9/5/2016  AlexNet_Unmodified.py    CaffeNet  Caffe version  Added 11/5/2016  CaffeNet.py    VGG-19  Caffe Version  Added 9/5/2016  VGG-19.py    GoogLeNet  As described in Original paper  Added 11/5/2016     The next intention is to check if I can setup Tensorflow distributed training and Tensorflow Serving, untrained models will be used for this.", 
            "title": "Intention"
        }, 
        {
            "location": "/#dependencies", 
            "text": "Keras  Theano / Tensorflow  Matplotlib  Numpy  Pydot", 
            "title": "Dependencies"
        }, 
        {
            "location": "/#directory-structure", 
            "text": "/ImageNetModels\n    /KerasLayers\n        Custom_Layers.py - Contains LRN2D layer\n    /Model\n        *.png - pydot visulisations of each of the models\n        *.txt - outputs from building the models\n    /Serving - reserved for the exported Tensorflow models\n    /docs - MKDocs files\n    *.py - Keras Models", 
            "title": "Directory Structure"
        }, 
        {
            "location": "/alexnet/", 
            "text": "Info\n#\n\n\nTwo version of the AlexNet model have been created:\n\n\n\n\nCaffe Pre-trained version\n\n\nthe version displayed in the diagram from the AlexNet paper\n\n\n\n\n@article{ding2014theano,\n  title={Theano-based Large-Scale Visual Recognition with Multiple GPUs},\n  author={Ding, Weiguang and Wang, Ruoyan and Mao, Fei and Taylor, Graham},\n  journal={arXiv preprint arXiv:1412.2302},\n  year={2014}\n}\n\n\n\n\nKeras Model Visulisation\n#\n\n\nAlexNet (CaffeNet version )\n\n\n\n\nAlexNet (Original)\n\n\n\n\nKeras Model Builds\n#\n\n\nAlexNet (CaffeNet version )\n\n\n# Channel 1 - Convolution Net Layer 1\nx = conv2D_lrn2d(\n    img_input, 3, 11, 11, subsample=(\n        1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        4, 4), pool_size=(\n            4, 4), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 2\nx = conv2D_lrn2d(x, 48, 55, 55, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 3\nx = conv2D_lrn2d(x, 128, 27, 27, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 4\nx = conv2D_lrn2d(x, 192, 13, 13, subsample=(1, 1), border_mode='same')\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 5\nx = conv2D_lrn2d(x, 192, 13, 13, subsample=(1, 1), border_mode='same')\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Cov Net Layer 6\nx = conv2D_lrn2d(x, 128, 27, 27, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Cov Net Layer 7\nx = Flatten()(x)\nx = Dense(2048, activation='relu')(x)\nx = Dropout(DROPOUT)(x)\n\n# Channel 1 - Cov Net Layer 8\nx = Dense(2048, activation='relu')(x)\nx = Dropout(DROPOUT)(x)\n\n# Final Channel - Cov Net 9\nx = Dense(output_dim=NB_CLASS,\n          activation='softmax')(x)\n\n\n\nAlexNet (Original)\n\n\n# Channel 1 - Conv Net Layer 1\nx = conv2D_bn(img_input, 3, 11, 11, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        4, 4), pool_size=(\n            4, 4), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 2 - Conv Net Layer 1\ny = conv2D_bn(img_input, 3, 11, 11, subsample=(1, 1), border_mode='same')\ny = MaxPooling2D(\n    strides=(\n        4, 4), pool_size=(\n            4, 4), dim_ordering=DIM_ORDERING)(y)\ny = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(y)\n\n# Channel 1 - Conv Net Layer 2\nx = conv2D_bn(x, 48, 55, 55, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 2 - Conv Net Layer 2\ny = conv2D_bn(y, 48, 55, 55, subsample=(1, 1), border_mode='same')\ny = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(y)\ny = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(y)\n\n# Channel 1 - Conv Net Layer 3\nx = conv2D_bn(x, 128, 27, 27, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 2 - Conv Net Layer 3\ny = conv2D_bn(y, 128, 27, 27, subsample=(1, 1), border_mode='same')\ny = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(y)\ny = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(y)\n\n# Channel 1 - Conv Net Layer 4\nx1 = merge([x, y], mode='concat', concat_axis=CONCAT_AXIS)\nx1 = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x1)\nx1 = conv2D_bn(x1, 192, 13, 13, subsample=(1, 1), border_mode='same')\n\n# Channel 2 - Conv Net Layer 4\ny1 = merge([x, y], mode='concat', concat_axis=CONCAT_AXIS)\ny1 = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(y1)\ny1 = conv2D_bn(y1, 192, 13, 13, subsample=(1, 1), border_mode='same')\n\n# Channel 1 - Conv Net Layer 5\nx2 = merge([x1, y1], mode='concat', concat_axis=CONCAT_AXIS)\nx2 = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x2)\nx2 = conv2D_bn(x2, 192, 13, 13, subsample=(1, 1), border_mode='same')\n\n# Channel 2 - Conv Net Layer 5\ny2 = merge([x1, y1], mode='concat', concat_axis=CONCAT_AXIS)\ny2 = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(y2)\ny2 = conv2D_bn(y2, 192, 13, 13, subsample=(1, 1), border_mode='same')\n\n# Channel 1 - Cov Net Layer 6\nx3 = conv2D_bn(x2, 128, 27, 27, subsample=(1, 1), border_mode='same')\nx3 = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x3)\nx3 = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x3)\n\n# Channel 2 - Cov Net Layer 6\ny3 = conv2D_bn(y2, 128, 27, 27, subsample=(1, 1), border_mode='same')\ny3 = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(y3)\ny3 = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(y3)\n\n# Channel 1 - Cov Net Layer 7\nx4 = merge([x3, y3], mode='mul', concat_axis=CONCAT_AXIS)\nx4 = Flatten()(x4)\nx4 = Dense(2048, activation='relu')(x4)\nx4 = Dropout(DROPOUT)(x4)\n\n# Channel 2 - Cov Net Layer 7\ny4 = merge([x3, y3], mode='mul', concat_axis=CONCAT_AXIS)\ny4 = Flatten()(y4)\ny4 = Dense(2048, activation='relu')(y4)\ny4 = Dropout(DROPOUT)(y4)\n\n# Channel 1 - Cov Net Layer 8\nx5 = merge([x4, y4], mode='mul')\nx5 = Dense(2048, activation='relu')(x5)\nx5 = Dropout(DROPOUT)(x5)\n\n# Channel 2 - Cov Net Layer 8\ny5 = merge([x4, y4], mode='mul')\ny5 = Dense(2048, activation='relu')(y5)\ny5 = Dropout(DROPOUT)(y5)\n\n# Final Channel - Cov Net 9\nxy = merge([x5, y5], mode='mul')\nxy = Dense(output_dim=NB_CLASS,\n           activation='softmax')(xy)", 
            "title": "AlexNet"
        }, 
        {
            "location": "/alexnet/#info", 
            "text": "Two version of the AlexNet model have been created:   Caffe Pre-trained version  the version displayed in the diagram from the AlexNet paper   @article{ding2014theano,\n  title={Theano-based Large-Scale Visual Recognition with Multiple GPUs},\n  author={Ding, Weiguang and Wang, Ruoyan and Mao, Fei and Taylor, Graham},\n  journal={arXiv preprint arXiv:1412.2302},\n  year={2014}\n}", 
            "title": "Info"
        }, 
        {
            "location": "/alexnet/#keras-model-visulisation", 
            "text": "AlexNet (CaffeNet version )   AlexNet (Original)", 
            "title": "Keras Model Visulisation"
        }, 
        {
            "location": "/alexnet/#keras-model-builds", 
            "text": "AlexNet (CaffeNet version )  # Channel 1 - Convolution Net Layer 1\nx = conv2D_lrn2d(\n    img_input, 3, 11, 11, subsample=(\n        1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        4, 4), pool_size=(\n            4, 4), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 2\nx = conv2D_lrn2d(x, 48, 55, 55, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 3\nx = conv2D_lrn2d(x, 128, 27, 27, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 4\nx = conv2D_lrn2d(x, 192, 13, 13, subsample=(1, 1), border_mode='same')\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 5\nx = conv2D_lrn2d(x, 192, 13, 13, subsample=(1, 1), border_mode='same')\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Cov Net Layer 6\nx = conv2D_lrn2d(x, 128, 27, 27, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Cov Net Layer 7\nx = Flatten()(x)\nx = Dense(2048, activation='relu')(x)\nx = Dropout(DROPOUT)(x)\n\n# Channel 1 - Cov Net Layer 8\nx = Dense(2048, activation='relu')(x)\nx = Dropout(DROPOUT)(x)\n\n# Final Channel - Cov Net 9\nx = Dense(output_dim=NB_CLASS,\n          activation='softmax')(x)  AlexNet (Original)  # Channel 1 - Conv Net Layer 1\nx = conv2D_bn(img_input, 3, 11, 11, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        4, 4), pool_size=(\n            4, 4), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 2 - Conv Net Layer 1\ny = conv2D_bn(img_input, 3, 11, 11, subsample=(1, 1), border_mode='same')\ny = MaxPooling2D(\n    strides=(\n        4, 4), pool_size=(\n            4, 4), dim_ordering=DIM_ORDERING)(y)\ny = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(y)\n\n# Channel 1 - Conv Net Layer 2\nx = conv2D_bn(x, 48, 55, 55, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 2 - Conv Net Layer 2\ny = conv2D_bn(y, 48, 55, 55, subsample=(1, 1), border_mode='same')\ny = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(y)\ny = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(y)\n\n# Channel 1 - Conv Net Layer 3\nx = conv2D_bn(x, 128, 27, 27, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 2 - Conv Net Layer 3\ny = conv2D_bn(y, 128, 27, 27, subsample=(1, 1), border_mode='same')\ny = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(y)\ny = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(y)\n\n# Channel 1 - Conv Net Layer 4\nx1 = merge([x, y], mode='concat', concat_axis=CONCAT_AXIS)\nx1 = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x1)\nx1 = conv2D_bn(x1, 192, 13, 13, subsample=(1, 1), border_mode='same')\n\n# Channel 2 - Conv Net Layer 4\ny1 = merge([x, y], mode='concat', concat_axis=CONCAT_AXIS)\ny1 = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(y1)\ny1 = conv2D_bn(y1, 192, 13, 13, subsample=(1, 1), border_mode='same')\n\n# Channel 1 - Conv Net Layer 5\nx2 = merge([x1, y1], mode='concat', concat_axis=CONCAT_AXIS)\nx2 = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x2)\nx2 = conv2D_bn(x2, 192, 13, 13, subsample=(1, 1), border_mode='same')\n\n# Channel 2 - Conv Net Layer 5\ny2 = merge([x1, y1], mode='concat', concat_axis=CONCAT_AXIS)\ny2 = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(y2)\ny2 = conv2D_bn(y2, 192, 13, 13, subsample=(1, 1), border_mode='same')\n\n# Channel 1 - Cov Net Layer 6\nx3 = conv2D_bn(x2, 128, 27, 27, subsample=(1, 1), border_mode='same')\nx3 = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x3)\nx3 = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x3)\n\n# Channel 2 - Cov Net Layer 6\ny3 = conv2D_bn(y2, 128, 27, 27, subsample=(1, 1), border_mode='same')\ny3 = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(y3)\ny3 = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(y3)\n\n# Channel 1 - Cov Net Layer 7\nx4 = merge([x3, y3], mode='mul', concat_axis=CONCAT_AXIS)\nx4 = Flatten()(x4)\nx4 = Dense(2048, activation='relu')(x4)\nx4 = Dropout(DROPOUT)(x4)\n\n# Channel 2 - Cov Net Layer 7\ny4 = merge([x3, y3], mode='mul', concat_axis=CONCAT_AXIS)\ny4 = Flatten()(y4)\ny4 = Dense(2048, activation='relu')(y4)\ny4 = Dropout(DROPOUT)(y4)\n\n# Channel 1 - Cov Net Layer 8\nx5 = merge([x4, y4], mode='mul')\nx5 = Dense(2048, activation='relu')(x5)\nx5 = Dropout(DROPOUT)(x5)\n\n# Channel 2 - Cov Net Layer 8\ny5 = merge([x4, y4], mode='mul')\ny5 = Dense(2048, activation='relu')(y5)\ny5 = Dropout(DROPOUT)(y5)\n\n# Final Channel - Cov Net 9\nxy = merge([x5, y5], mode='mul')\nxy = Dense(output_dim=NB_CLASS,\n           activation='softmax')(xy)", 
            "title": "Keras Model Builds"
        }, 
        {
            "location": "/caffenet/", 
            "text": "Info\n#\n\n\nOnly one version of CaffeNet has been built\n\n\n@article{ding2014theano,\n  title={Theano-based Large-Scale Visual Recognition with Multiple GPUs},\n  author={Ding, Weiguang and Wang, Ruoyan and Mao, Fei and Taylor, Graham},\n  journal={arXiv preprint arXiv:1412.2302},\n  year={2014}\n}\n\n\n\n\nKeras Model Visulisation\n#\n\n\nCaffeNet\n\n\n\n\nKeras Model Builds\n#\n\n\n# Channel 1 - Convolution Net Input Layer\nx = conv2D_lrn2d(\n    img_input, 3, 11, 11, subsample=(\n        1, 1), border_mode='same')\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 1\nx = conv2D_lrn2d(x, 96, 55, 55, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\nx = LRN2D(alpha=ALPHA, beta=BETA)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 2\nx = conv2D_lrn2d(x, 192, 27, 27, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\nx = LRN2D(alpha=ALPHA, beta=BETA)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 3\nx = conv2D_lrn2d(x, 288, 13, 13, subsample=(1, 1), border_mode='same')\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 4\nx = conv2D_lrn2d(x, 288, 13, 13, subsample=(1, 1), border_mode='same')\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 5\nx = conv2D_lrn2d(x, 256, 13, 13, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Cov Net Layer 7\nx = Flatten()(x)\nx = Dense(4096, activation='relu')(x)\nx = Dropout(DROPOUT)(x)\n\n# Channel 1 - Cov Net Layer 8\nx = Dense(4096, activation='relu')(x)\nx = Dropout(DROPOUT)(x)\n\n# Final Channel - Cov Net 9\nx = Dense(output_dim=NB_CLASS,\n          activation='softmax')(x)", 
            "title": "CaffeNet"
        }, 
        {
            "location": "/caffenet/#info", 
            "text": "Only one version of CaffeNet has been built  @article{ding2014theano,\n  title={Theano-based Large-Scale Visual Recognition with Multiple GPUs},\n  author={Ding, Weiguang and Wang, Ruoyan and Mao, Fei and Taylor, Graham},\n  journal={arXiv preprint arXiv:1412.2302},\n  year={2014}\n}", 
            "title": "Info"
        }, 
        {
            "location": "/caffenet/#keras-model-visulisation", 
            "text": "CaffeNet", 
            "title": "Keras Model Visulisation"
        }, 
        {
            "location": "/caffenet/#keras-model-builds", 
            "text": "# Channel 1 - Convolution Net Input Layer\nx = conv2D_lrn2d(\n    img_input, 3, 11, 11, subsample=(\n        1, 1), border_mode='same')\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 1\nx = conv2D_lrn2d(x, 96, 55, 55, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\nx = LRN2D(alpha=ALPHA, beta=BETA)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 2\nx = conv2D_lrn2d(x, 192, 27, 27, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\nx = LRN2D(alpha=ALPHA, beta=BETA)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 3\nx = conv2D_lrn2d(x, 288, 13, 13, subsample=(1, 1), border_mode='same')\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 4\nx = conv2D_lrn2d(x, 288, 13, 13, subsample=(1, 1), border_mode='same')\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Convolution Net Layer 5\nx = conv2D_lrn2d(x, 256, 13, 13, subsample=(1, 1), border_mode='same')\nx = MaxPooling2D(\n    strides=(\n        2, 2), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\n# Channel 1 - Cov Net Layer 7\nx = Flatten()(x)\nx = Dense(4096, activation='relu')(x)\nx = Dropout(DROPOUT)(x)\n\n# Channel 1 - Cov Net Layer 8\nx = Dense(4096, activation='relu')(x)\nx = Dropout(DROPOUT)(x)\n\n# Final Channel - Cov Net 9\nx = Dense(output_dim=NB_CLASS,\n          activation='softmax')(x)", 
            "title": "Keras Model Builds"
        }, 
        {
            "location": "/googlenet/", 
            "text": "Info\n#\n\n\nOnly one version of CaffeNet has been built\n\n\nGoing deeper with convolutions\nSzegedy, Christian; Liu, Wei; Jia, Yangqing; Sermanet, Pierre; Reed, Scott; Anguelov, Dragomir;\nErhan, Dumitru; Vanhoucke, Vincent; Rabinovich, Andrew\narXiv:1409.4842\n\n\n\n\nKeras Model Visulisation\n#\n\n\nGoogLeNet\n\n\n\n\nKeras Model Builds\n#\n\n\nInception\n\n\ndef inception_module(x, params, dim_ordering, concat_axis,\n                     subsample=(1, 1), activation='relu',\n                     border_mode='same', weight_decay=None):\n\n    # https://gist.github.com/nervanazoo/2e5be01095e935e90dd8  #\n    # file-googlenet_neon-py\n\n    (branch1, branch2, branch3, branch4) = params\n\n    if weight_decay:\n        W_regularizer = regularizers.l2(weight_decay)\n        b_regularizer = regularizers.l2(weight_decay)\n    else:\n        W_regularizer = None\n        b_regularizer = None\n\n    pathway1 = Convolution2D(branch1[0], 1, 1,\n                             subsample=subsample,\n                             activation=activation,\n                             border_mode=border_mode,\n                             W_regularizer=W_regularizer,\n                             b_regularizer=b_regularizer,\n                             bias=False,\n                             dim_ordering=dim_ordering)(x)\n\n    pathway2 = Convolution2D(branch2[0], 1, 1,\n                             subsample=subsample,\n                             activation=activation,\n                             border_mode=border_mode,\n                             W_regularizer=W_regularizer,\n                             b_regularizer=b_regularizer,\n                             bias=False,\n                             dim_ordering=dim_ordering)(x)\n    pathway2 = Convolution2D(branch2[1], 3, 3,\n                             subsample=subsample,\n                             activation=activation,\n                             border_mode=border_mode,\n                             W_regularizer=W_regularizer,\n                             b_regularizer=b_regularizer,\n                             bias=False,\n                             dim_ordering=dim_ordering)(pathway2)\n\n    pathway3 = Convolution2D(branch3[0], 1, 1,\n                             subsample=subsample,\n                             activation=activation,\n                             border_mode=border_mode,\n                             W_regularizer=W_regularizer,\n                             b_regularizer=b_regularizer,\n                             bias=False,\n                             dim_ordering=dim_ordering)(x)\n    pathway3 = Convolution2D(branch3[1], 5, 5,\n                             subsample=subsample,\n                             activation=activation,\n                             border_mode=border_mode,\n                             W_regularizer=W_regularizer,\n                             b_regularizer=b_regularizer,\n                             bias=False,\n                             dim_ordering=dim_ordering)(pathway3)\n\n    pathway4 = MaxPooling2D(pool_size=(1, 1), dim_ordering=DIM_ORDERING)(x)\n    pathway4 = Convolution2D(branch4[0], 1, 1,\n                             subsample=subsample,\n                             activation=activation,\n                             border_mode=border_mode,\n                             W_regularizer=W_regularizer,\n                             b_regularizer=b_regularizer,\n                             bias=False,\n                             dim_ordering=dim_ordering)(pathway4)\n\n    return merge([pathway1, pathway2, pathway3, pathway4],\n                 mode='concat', concat_axis=concat_axis)\n\n\n\nModel\n\n\nx = conv_layer(img_input, nb_col=7, nb_filter=64,\n               nb_row=7, dim_ordering=DIM_ORDERING, padding=3)\nx = MaxPooling2D(\n    strides=(\n        3, 3), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\n\nx = conv_layer(x, nb_col=1, nb_filter=64,\n               nb_row=1, dim_ordering=DIM_ORDERING)\nx = conv_layer(x, nb_col=3, nb_filter=192,\n               nb_row=3, dim_ordering=DIM_ORDERING, padding=1)\nx = MaxPooling2D(\n    strides=(\n        3, 3), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\n\nx = inception_module(x, params=[(64, ), (96, 128), (16, 32), (32, )],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\nx = inception_module(x, params=[(128,), (128, 192), (32, 96), (64, )],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\n\nx = MaxPooling2D(\n    strides=(\n        1, 1), pool_size=(\n            1, 1), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\nx = inception_module(x, params=[(192,), (96, 208), (16, 48), (64, )],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\n# AUX 1 - Branch HERE\nx = inception_module(x, params=[(160,), (112, 224), (24, 64), (64, )],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\nx = inception_module(x, params=[(128,), (128, 256), (24, 64), (64, )],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\nx = inception_module(x, params=[(112,), (144, 288), (32, 64), (64, )],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\n# AUX 2 - Branch HERE\nx = inception_module(x, params=[(256,), (160, 320), (32, 128), (128,)],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\nx = MaxPooling2D(\n    strides=(\n        1, 1), pool_size=(\n            1, 1), dim_ordering=DIM_ORDERING)(x)\n\nx = inception_module(x, params=[(256,), (160, 320), (32, 128), (128,)],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\nx = inception_module(x, params=[(384,), (192, 384), (48, 128), (128,)],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\nx = AveragePooling2D(strides=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Flatten()(x)\nx = Dropout(DROPOUT)(x)\nx = Dense(output_dim=NB_CLASS,\n          activation='linear')(x)\nx = Dense(output_dim=NB_CLASS,\n          activation='softmax')(x)", 
            "title": "GoogLeNet"
        }, 
        {
            "location": "/googlenet/#info", 
            "text": "Only one version of CaffeNet has been built  Going deeper with convolutions\nSzegedy, Christian; Liu, Wei; Jia, Yangqing; Sermanet, Pierre; Reed, Scott; Anguelov, Dragomir;\nErhan, Dumitru; Vanhoucke, Vincent; Rabinovich, Andrew\narXiv:1409.4842", 
            "title": "Info"
        }, 
        {
            "location": "/googlenet/#keras-model-visulisation", 
            "text": "GoogLeNet", 
            "title": "Keras Model Visulisation"
        }, 
        {
            "location": "/googlenet/#keras-model-builds", 
            "text": "Inception  def inception_module(x, params, dim_ordering, concat_axis,\n                     subsample=(1, 1), activation='relu',\n                     border_mode='same', weight_decay=None):\n\n    # https://gist.github.com/nervanazoo/2e5be01095e935e90dd8  #\n    # file-googlenet_neon-py\n\n    (branch1, branch2, branch3, branch4) = params\n\n    if weight_decay:\n        W_regularizer = regularizers.l2(weight_decay)\n        b_regularizer = regularizers.l2(weight_decay)\n    else:\n        W_regularizer = None\n        b_regularizer = None\n\n    pathway1 = Convolution2D(branch1[0], 1, 1,\n                             subsample=subsample,\n                             activation=activation,\n                             border_mode=border_mode,\n                             W_regularizer=W_regularizer,\n                             b_regularizer=b_regularizer,\n                             bias=False,\n                             dim_ordering=dim_ordering)(x)\n\n    pathway2 = Convolution2D(branch2[0], 1, 1,\n                             subsample=subsample,\n                             activation=activation,\n                             border_mode=border_mode,\n                             W_regularizer=W_regularizer,\n                             b_regularizer=b_regularizer,\n                             bias=False,\n                             dim_ordering=dim_ordering)(x)\n    pathway2 = Convolution2D(branch2[1], 3, 3,\n                             subsample=subsample,\n                             activation=activation,\n                             border_mode=border_mode,\n                             W_regularizer=W_regularizer,\n                             b_regularizer=b_regularizer,\n                             bias=False,\n                             dim_ordering=dim_ordering)(pathway2)\n\n    pathway3 = Convolution2D(branch3[0], 1, 1,\n                             subsample=subsample,\n                             activation=activation,\n                             border_mode=border_mode,\n                             W_regularizer=W_regularizer,\n                             b_regularizer=b_regularizer,\n                             bias=False,\n                             dim_ordering=dim_ordering)(x)\n    pathway3 = Convolution2D(branch3[1], 5, 5,\n                             subsample=subsample,\n                             activation=activation,\n                             border_mode=border_mode,\n                             W_regularizer=W_regularizer,\n                             b_regularizer=b_regularizer,\n                             bias=False,\n                             dim_ordering=dim_ordering)(pathway3)\n\n    pathway4 = MaxPooling2D(pool_size=(1, 1), dim_ordering=DIM_ORDERING)(x)\n    pathway4 = Convolution2D(branch4[0], 1, 1,\n                             subsample=subsample,\n                             activation=activation,\n                             border_mode=border_mode,\n                             W_regularizer=W_regularizer,\n                             b_regularizer=b_regularizer,\n                             bias=False,\n                             dim_ordering=dim_ordering)(pathway4)\n\n    return merge([pathway1, pathway2, pathway3, pathway4],\n                 mode='concat', concat_axis=concat_axis)  Model  x = conv_layer(img_input, nb_col=7, nb_filter=64,\n               nb_row=7, dim_ordering=DIM_ORDERING, padding=3)\nx = MaxPooling2D(\n    strides=(\n        3, 3), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\n\nx = conv_layer(x, nb_col=1, nb_filter=64,\n               nb_row=1, dim_ordering=DIM_ORDERING)\nx = conv_layer(x, nb_col=3, nb_filter=192,\n               nb_row=3, dim_ordering=DIM_ORDERING, padding=1)\nx = MaxPooling2D(\n    strides=(\n        3, 3), pool_size=(\n            2, 2), dim_ordering=DIM_ORDERING)(x)\n\nx = inception_module(x, params=[(64, ), (96, 128), (16, 32), (32, )],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\nx = inception_module(x, params=[(128,), (128, 192), (32, 96), (64, )],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\n\nx = MaxPooling2D(\n    strides=(\n        1, 1), pool_size=(\n            1, 1), dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\n\nx = inception_module(x, params=[(192,), (96, 208), (16, 48), (64, )],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\n# AUX 1 - Branch HERE\nx = inception_module(x, params=[(160,), (112, 224), (24, 64), (64, )],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\nx = inception_module(x, params=[(128,), (128, 256), (24, 64), (64, )],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\nx = inception_module(x, params=[(112,), (144, 288), (32, 64), (64, )],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\n# AUX 2 - Branch HERE\nx = inception_module(x, params=[(256,), (160, 320), (32, 128), (128,)],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\nx = MaxPooling2D(\n    strides=(\n        1, 1), pool_size=(\n            1, 1), dim_ordering=DIM_ORDERING)(x)\n\nx = inception_module(x, params=[(256,), (160, 320), (32, 128), (128,)],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\nx = inception_module(x, params=[(384,), (192, 384), (48, 128), (128,)],\n                     dim_ordering=DIM_ORDERING, concat_axis=CONCAT_AXIS)\nx = AveragePooling2D(strides=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Flatten()(x)\nx = Dropout(DROPOUT)(x)\nx = Dense(output_dim=NB_CLASS,\n          activation='linear')(x)\nx = Dense(output_dim=NB_CLASS,\n          activation='softmax')(x)", 
            "title": "Keras Model Builds"
        }, 
        {
            "location": "/vgg19/", 
            "text": "Info\n#\n\n\nOnly one version of VGG-19 has been built\n\n\n@article{DBLP:journals/corr/SimonyanZ14a,\n      author    = {Karen Simonyan and\n                   Andrew Zisserman},\n      title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},\n      journal   = {CoRR},\n      volume    = {abs/1409.1556},\n      year      = {2014},\n      url       = {http://arxiv.org/abs/1409.1556},\n      timestamp = {Wed, 01 Oct 2014 15:00:05 +0200},\n      biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SimonyanZ14a},\n      bibsource = {dblp computer science bibliography, http://dblp.org}\n    }\n\n\n\n\nKeras Model Visulisation\n#\n\n\nVGG-19\n\n\n\n\nKeras Model Builds\n#\n\n\n# Layer Cluster - 1\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(img_input)\nx = Convolution2D(64, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(64, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = MaxPooling2D((2, 2), strides=(2, 2))(x)\n\n# Layer Cluster - 2\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(128, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(128, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = MaxPooling2D((2, 2), strides=(2, 2))(x)\n\n# Layer Cluster - 3\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(256, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(256, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(256, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(256, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = MaxPooling2D((2, 2), strides=(2, 2))(x)\n\n# Layer Cluster - 4\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(512, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(512, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(512, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(512, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = MaxPooling2D((2, 2), strides=(2, 2))(x)\n\n# Layer Cluster - 5 - Output Layer\nx = Flatten()(x)\nx = Dense(4096, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(4096, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(1000, activation='softmax')(x)", 
            "title": "VGG-19"
        }, 
        {
            "location": "/vgg19/#info", 
            "text": "Only one version of VGG-19 has been built  @article{DBLP:journals/corr/SimonyanZ14a,\n      author    = {Karen Simonyan and\n                   Andrew Zisserman},\n      title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},\n      journal   = {CoRR},\n      volume    = {abs/1409.1556},\n      year      = {2014},\n      url       = {http://arxiv.org/abs/1409.1556},\n      timestamp = {Wed, 01 Oct 2014 15:00:05 +0200},\n      biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SimonyanZ14a},\n      bibsource = {dblp computer science bibliography, http://dblp.org}\n    }", 
            "title": "Info"
        }, 
        {
            "location": "/vgg19/#keras-model-visulisation", 
            "text": "VGG-19", 
            "title": "Keras Model Visulisation"
        }, 
        {
            "location": "/vgg19/#keras-model-builds", 
            "text": "# Layer Cluster - 1\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(img_input)\nx = Convolution2D(64, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(64, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = MaxPooling2D((2, 2), strides=(2, 2))(x)\n\n# Layer Cluster - 2\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(128, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(128, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = MaxPooling2D((2, 2), strides=(2, 2))(x)\n\n# Layer Cluster - 3\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(256, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(256, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(256, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(256, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = MaxPooling2D((2, 2), strides=(2, 2))(x)\n\n# Layer Cluster - 4\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(512, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(512, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(512, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = ZeroPadding2D(padding=(1, 1), dim_ordering=DIM_ORDERING)(x)\nx = Convolution2D(512, 3, 3,\n                  activation='relu',\n                  border_mode='same',\n                  dim_ordering=DIM_ORDERING)(x)\nx = MaxPooling2D((2, 2), strides=(2, 2))(x)\n\n# Layer Cluster - 5 - Output Layer\nx = Flatten()(x)\nx = Dense(4096, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(4096, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(1000, activation='softmax')(x)", 
            "title": "Keras Model Builds"
        }, 
        {
            "location": "/demos/", 
            "text": "Info\n#\n\n\nThe demo script \nTrain.py\n can used to demo the following functions:\n\n\n*   Demonstrates that the Compile Model can process an Output\n\n*   Demonstrates who to export the Tensorflow Model (Issue)\n\n*   Demonstrates how to visualise slices of the Convolutional Layers\n\n*   Saves model summary and Image to the Model folder\n\n\n\nControllers\n#\n\n\nBy changing the following lines to True will dictate the what function is processed.\n\n\ntest_batch = False\nget_graph = False\nshow_activation = False\nshow_cmd_output = True\n\n\n\n\n\ntest_batch - this will compile the model, generate some random random data and then predict the output.\n\n\nget_graph - this will when it works how to export the Tensorflow graph\n\n\nshow_activation - this function displays using Matplotlib the visulisation of 10 slices from one Covnet Layer\n\n\nshow_cmd_output - this will save a summary of the file to txt file and export the diagram of the model to a folder. Instructions are contained in the file of how to run this.\n\n\n\n\nLayer Slices\n#", 
            "title": "Demos"
        }, 
        {
            "location": "/demos/#info", 
            "text": "The demo script  Train.py  can used to demo the following functions:  *   Demonstrates that the Compile Model can process an Output\n\n*   Demonstrates who to export the Tensorflow Model (Issue)\n\n*   Demonstrates how to visualise slices of the Convolutional Layers\n\n*   Saves model summary and Image to the Model folder", 
            "title": "Info"
        }, 
        {
            "location": "/demos/#controllers", 
            "text": "By changing the following lines to True will dictate the what function is processed.  test_batch = False\nget_graph = False\nshow_activation = False\nshow_cmd_output = True   test_batch - this will compile the model, generate some random random data and then predict the output.  get_graph - this will when it works how to export the Tensorflow graph  show_activation - this function displays using Matplotlib the visulisation of 10 slices from one Covnet Layer  show_cmd_output - this will save a summary of the file to txt file and export the diagram of the model to a folder. Instructions are contained in the file of how to run this.", 
            "title": "Controllers"
        }, 
        {
            "location": "/demos/#layer-slices", 
            "text": "", 
            "title": "Layer Slices"
        }, 
        {
            "location": "/license/", 
            "text": "Information\n#\n\n\nThe prototxt files are as they would be found on the Caffe Model zoo Github, used only as a meaningful reference for the build.\n\n\nThe data used to train this model comes from the ImageNet project, which distributes its database to researchers who agree to a following term of access: \"Researcher shall use the Database only for non-commercial research and educational purposes.\" Accordingly, this model is distributed under a non-commercial license.\n\n\nBVLC model license\n#\n\n\nThe Caffe models bundled by the BVLC are released for unrestricted use.\n\n\nThese models are trained on data from the ImageNet project and training data includes internet photos that may be subject to copyright.\n\n\nOur present understanding as researchers is that there is no restriction placed on the open release of these learned model weights, since none of the original images are distributed in whole or in part. To the extent that the interpretation arises that weights are derivative works of the original copyright holder and they assert such a copyright, UC Berkeley makes no representations as to what use is allowed other than to consider our present release in the spirit of fair use in the academic mission of the university to disseminate knowledge and tools as broadly as possible without restriction.\n\n\nAlexNet\n#\n\n\n@article{ding2014theano,\n  title={Theano-based Large-Scale Visual Recognition with Multiple GPUs},\n  author={Ding, Weiguang and Wang, Ruoyan and Mao, Fei and Taylor, Graham},\n  journal={arXiv preprint arXiv:1412.2302},\n  year={2014}\n}\n\n\n\nFramework used: Caffe\n\n\nLicense: BVLC model license - unrestricted use\n\n\nDataset used to train: ILSVRC-2012\n\n\nCaffeNet\n#\n\n\n@article{ding2014theano,\n  title={Theano-based Large-Scale Visual Recognition with Multiple GPUs},\n  author={Ding, Weiguang and Wang, Ruoyan and Mao, Fei and Taylor, Graham},\n  journal={arXiv preprint arXiv:1412.2302},\n  year={2014}\n}\n\n\n\nFramework used: Caffe\n\n\nLicense: BVLC model license - unrestricted use\n\n\nDataset used to train: ILSVRC-2012\n\n\nGoogLeNet\n#\n\n\nGoing deeper with convolutions\nSzegedy, Christian; Liu, Wei; Jia, Yangqing; Sermanet, Pierre; Reed, Scott; Anguelov, Dragomir;\nErhan, Dumitru; Vanhoucke, Vincent; Rabinovich, Andrew\narXiv:1409.4842\n\n\n\nThis is an implementation of the GoogLeNet model nearly as described in Szegedy et. al. 2014, see Table 1.\n\n\nVGG-19\n#\n\n\n@article{DBLP:journals/corr/SimonyanZ14a,\n  author    = {Karen Simonyan and\n               Andrew Zisserman},\n  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},\n  journal   = {CoRR},\n  volume    = {abs/1409.1556},\n  year      = {2014},\n  url       = {http://arxiv.org/abs/1409.1556},\n  timestamp = {Wed, 01 Oct 2014 15:00:05 +0200},\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SimonyanZ14a},\n  bibsource = {dblp computer science bibliography, http://dblp.org}\n}\n\n\n\nFramework used: Caffe\n\n\nLicense: BVLC model license - unrestricted use\n\n\nDataset used to train: ILSVRC-2014", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/license/#information", 
            "text": "The prototxt files are as they would be found on the Caffe Model zoo Github, used only as a meaningful reference for the build.  The data used to train this model comes from the ImageNet project, which distributes its database to researchers who agree to a following term of access: \"Researcher shall use the Database only for non-commercial research and educational purposes.\" Accordingly, this model is distributed under a non-commercial license.", 
            "title": "Information"
        }, 
        {
            "location": "/license/#bvlc-model-license", 
            "text": "The Caffe models bundled by the BVLC are released for unrestricted use.  These models are trained on data from the ImageNet project and training data includes internet photos that may be subject to copyright.  Our present understanding as researchers is that there is no restriction placed on the open release of these learned model weights, since none of the original images are distributed in whole or in part. To the extent that the interpretation arises that weights are derivative works of the original copyright holder and they assert such a copyright, UC Berkeley makes no representations as to what use is allowed other than to consider our present release in the spirit of fair use in the academic mission of the university to disseminate knowledge and tools as broadly as possible without restriction.", 
            "title": "BVLC model license"
        }, 
        {
            "location": "/license/#alexnet", 
            "text": "@article{ding2014theano,\n  title={Theano-based Large-Scale Visual Recognition with Multiple GPUs},\n  author={Ding, Weiguang and Wang, Ruoyan and Mao, Fei and Taylor, Graham},\n  journal={arXiv preprint arXiv:1412.2302},\n  year={2014}\n}  Framework used: Caffe  License: BVLC model license - unrestricted use  Dataset used to train: ILSVRC-2012", 
            "title": "AlexNet"
        }, 
        {
            "location": "/license/#caffenet", 
            "text": "@article{ding2014theano,\n  title={Theano-based Large-Scale Visual Recognition with Multiple GPUs},\n  author={Ding, Weiguang and Wang, Ruoyan and Mao, Fei and Taylor, Graham},\n  journal={arXiv preprint arXiv:1412.2302},\n  year={2014}\n}  Framework used: Caffe  License: BVLC model license - unrestricted use  Dataset used to train: ILSVRC-2012", 
            "title": "CaffeNet"
        }, 
        {
            "location": "/license/#googlenet", 
            "text": "Going deeper with convolutions\nSzegedy, Christian; Liu, Wei; Jia, Yangqing; Sermanet, Pierre; Reed, Scott; Anguelov, Dragomir;\nErhan, Dumitru; Vanhoucke, Vincent; Rabinovich, Andrew\narXiv:1409.4842  This is an implementation of the GoogLeNet model nearly as described in Szegedy et. al. 2014, see Table 1.", 
            "title": "GoogLeNet"
        }, 
        {
            "location": "/license/#vgg-19", 
            "text": "@article{DBLP:journals/corr/SimonyanZ14a,\n  author    = {Karen Simonyan and\n               Andrew Zisserman},\n  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},\n  journal   = {CoRR},\n  volume    = {abs/1409.1556},\n  year      = {2014},\n  url       = {http://arxiv.org/abs/1409.1556},\n  timestamp = {Wed, 01 Oct 2014 15:00:05 +0200},\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SimonyanZ14a},\n  bibsource = {dblp computer science bibliography, http://dblp.org}\n}  Framework used: Caffe  License: BVLC model license - unrestricted use  Dataset used to train: ILSVRC-2014", 
            "title": "VGG-19"
        }
    ]
}